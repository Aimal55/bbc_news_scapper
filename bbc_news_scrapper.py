# -*- coding: utf-8 -*-
"""BBC_news_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lYVOhzsi8pYnWFRvNENUcDIJi4rmIgn
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from google.colab import files

# Function to get page content
def get_page(url):
    response = requests.get(url)
    return response.content


def parse_news_articles(url):
    page_content = get_page(url)
    soup = BeautifulSoup(page_content, 'html.parser')

    \
    card_wrappers = soup.find_all('div', {'data-testid': 'card-text-wrapper'})  # Use the wrapper


    articles = []

    base_url = 'https://www.bbc.com'  # Base URL to prepend

    for wrapper in card_wrappers:
        # Extract headline (using data-testid="card-headline")
        headline = wrapper.find(attrs={"data-testid": "card-headline"})
        headline_text = headline.text.strip() if headline else "No Data"

        # Extract summary (using data-testid="card-description")
        summary = wrapper.find(attrs={"data-testid": "card-description"})
        summary_text = summary.text.strip() if summary else "No Data"

        # Extract the publication date (using data-testid="card-metadata-lastupdated")
        date = wrapper.find(attrs={"data-testid": "card-metadata-lastupdated"})
        date_text = date.text.strip() if date else "No Data"

        # Extract the news source (using meta tag with data-testid="card-metadata-tag")
        source_meta = wrapper.find(attrs={"data-testid": "card-metadata-tag"})
        source_text = source_meta.text.strip() if source_meta else "No Data"

        # Skip link extraction (not needed)
        # Add the article (headline, summary, date, source) to the articles list
        articles.append([headline_text, summary_text, date_text, source_text])

    return articles

# URL to parse
url = "https://www.bbc.com/news"
articles = parse_news_articles(url)

# Create a DataFrame without the "Link" column
df = pd.DataFrame(articles, columns=["Headline", "Summary", "Date", "Source"])


# Save the DataFrame to an Excel file with openpyxl
with pd.ExcelWriter("bbc_data.xlsx", engine="openpyxl") as writer:
    df.to_excel(writer, index=False, sheet_name="Sheet1")

    # Get the workbook and sheet
    workbook = writer.book
    sheet = workbook["Sheet1"]

    # Adjust column width
    for column in sheet.columns:
        max_length = 0
        # Get the cell values (keep original column object)
        cell_values = [str(cell.value) for cell in column]
        max_length = max(len(cell) for cell in cell_values)  # Get the max length of each column
        adjusted_width = (max_length + 1)  # Add padding for aesthetics
        # Use the original column object to access column_letter
        sheet.column_dimensions[column[0].column_letter].width = adjusted_width

# Trigger the download of the Excel file
files.download("bbc_data.xlsx")